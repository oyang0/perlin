{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044912d0-65d2-464b-94e7-cd17b832f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perlin import rand_perlin_2d\n",
    "from perlin import rand_perlin_2d_octaves\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from random import uniform\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.nn.init import orthogonal_\n",
    "from torch.nn.init import constant_\n",
    "\n",
    "from torchvision.transforms import RandomCrop\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import PILToTensor\n",
    "\n",
    "to_pil_image = ToPILImage()\n",
    "to_tensor = ToTensor()\n",
    "pil_to_tensor = PILToTensor()\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401df456-5847-454e-96d8-fe654147173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth=17,\n",
    "        n_channels=64,\n",
    "        image_channels=1,\n",
    "        kernel_size=3,\n",
    "        padding=1,\n",
    "    ):\n",
    "        super(DnCNN, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv2d(\n",
    "                image_channels,\n",
    "                n_channels,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                stride=(1, 1),\n",
    "                padding=(padding, padding),\n",
    "                bias=True,\n",
    "            )\n",
    "        )\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(depth - 2):\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    n_channels,\n",
    "                    n_channels,\n",
    "                    kernel_size=(kernel_size, kernel_size),\n",
    "                    stride=(1, 1),\n",
    "                    padding=(padding, padding),\n",
    "                    bias=True,\n",
    "                )\n",
    "            )\n",
    "            layers.append(\n",
    "                nn.BatchNorm2d(\n",
    "                    n_channels,\n",
    "                    eps=1e-05,\n",
    "                    momentum=0.1,\n",
    "                    affine=True,\n",
    "                    track_running_stats=True,\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(\n",
    "            nn.Conv2d(\n",
    "                n_channels,\n",
    "                image_channels,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                stride=(1, 1),\n",
    "                padding=(padding, padding),\n",
    "                bias=True,\n",
    "            )\n",
    "        )\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                orthogonal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                constant_(m.weight, 1)\n",
    "                constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def get_noise(image, level=None):\n",
    "    _, height, width = image.shape\n",
    "    shape = (height, width)\n",
    "    \n",
    "    if level is None:\n",
    "        level = uniform(-1, 1)\n",
    "    \n",
    "    if res is None:\n",
    "        hght_res = choice([i for i in range(1, height + 1) if height % i == 0])\n",
    "        wdth_res = choice([i for i in range(1, width + 1) if width % i == 0])\n",
    "        res = (hght_res, wdth_res)\n",
    "    \n",
    "    black = torch.tensor(0.0, dtype=image.dtype).to(image.device)\n",
    "    white = torch.tensor(1.0, dtype=image.dtype).to(image.device)\n",
    "\n",
    "    noise = rand_perlin_2d(shape, res).to(image.dtype).to(image.device)\n",
    "    noise = torch.where(noise < level, black, white)\n",
    "    noise = torch.where(image == 0.0, white, noise)\n",
    "\n",
    "    return noise\n",
    "\n",
    "\n",
    "def get_observation(image, noise):\n",
    "    noise = 1 - noise\n",
    "    observation = image - noise\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3168bd-a4d0-4ef6-80c0-27c921aaedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DnCNN.\n",
    "mdl_path = 'models/model-513.pth'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DnCNN(depth=30).eval().to(device)\n",
    "model.load_state_dict(torch.load(mdl_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8a694-dfdf-4b91-a379-f6a8e06853ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a clean image, noisy observation, and pepper noise.\n",
    "img_dir = 'datasets/test'\n",
    "img_nm = 'TIP BRISQUE-01.png'\n",
    "nis_lvl = -0.5\n",
    "hght_res = 440\n",
    "wdth_res = 425\n",
    "\n",
    "img_path = join(img_dir, img_nm)\n",
    "image = to_tensor(Image.open(img_path).convert('1'))\n",
    "noise = get_noise(image, level=nis_lvl, res=(hght_res, wdth_res))\n",
    "observation = get_observation(image, noise)\n",
    "\n",
    "to_pil_image(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63dba98-c472-4ad3-9c16-d090b7789c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the denoised image and residual image.\n",
    "with torch.no_grad():\n",
    "    pred = model(observation.unsqueeze(0))\n",
    "    pred = nn.Sigmoid()(pred)\n",
    "    pred = pred.round().squeeze(0)\n",
    "\n",
    "white = torch.tensor(1.0, dtype=image.dtype).to(image.device)\n",
    "residual = observation + (1 - pred)\n",
    "denoised = torch.where(observation == 0.0, residual, white)\n",
    "\n",
    "to_pil_image(denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e259e5a-712b-46e8-b080-d66bb46a3b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
